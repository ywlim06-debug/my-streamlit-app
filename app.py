# app.py
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ğŸª¨ ëŒë©©ì´ ê²°ì • ì½”ì¹˜ (Pebble Decision Coach) â€” Clean Tone Version
#
# Fix: ë§ˆì§€ë§‰ ì§ˆë¬¸ì´ ì´ì „ ì§ˆë¬¸ê³¼ ë°˜ë³µë˜ëŠ” í˜„ìƒ í•´ê²°
# - í”„ë¡¬í”„íŠ¸ì— "ì´ì „ ì§ˆë¬¸ê³¼ ì¤‘ë³µ ê¸ˆì§€" + ì´ì „ ì§ˆë¬¸ ëª©ë¡ ì œê³µ
# - ì¤‘ë³µ ê°ì§€ ì‹œ ìë™ ì¬ìƒì„± 1íšŒ(ëœë¤ nonce ì¶”ê°€)
# - ê·¸ë˜ë„ ì¤‘ë³µì´ë©´ ì½”ì¹˜ë³„ 'ì•ˆì „ ë§ˆì§€ë§‰ ì§ˆë¬¸'ìœ¼ë¡œ ëŒ€ì²´
#
# í•„ìš” íŒ¨í‚¤ì§€:
#   pip install streamlit openai
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

from __future__ import annotations

import base64
import json
import random
import textwrap
from datetime import datetime
from typing import Any, Dict, List, Optional, Tuple

import streamlit as st

try:
    from openai import OpenAI
except Exception:
    OpenAI = None  # type: ignore


# =========================
# Page Config
# =========================
st.set_page_config(page_title="ëŒë©©ì´ ê²°ì • ì½”ì¹˜", page_icon="ğŸª¨", layout="wide")

MODEL_PRIMARY = "gpt-5-mini"
MODEL_FALLBACK = "gpt-4o-mini"

TOPIC_CATEGORIES = [
    ("ğŸ“ í•™ì—…/ì§„ë¡œ", "í•™ì—…, ì „ê³µ ì„ íƒ, ì§„ë¡œ ë°©í–¥, ì·¨ì—…/ì´ì§, ëª©í‘œ ì„¤ì •"),
    ("ğŸ’¼ ì»¤ë¦¬ì–´/ì¼", "ì—…ë¬´ ì„ íƒ, í”„ë¡œì íŠ¸, í˜‘ì—…, ë¦¬ë”ì‹­, ì»¤ë¦¬ì–´ ì„±ì¥"),
    ("ğŸ’– ê´€ê³„", "ì¹œêµ¬/ì—°ì¸/ê°€ì¡±, ê°ˆë“±, ì†Œí†µ, ê±°ë¦¬ë‘ê¸°, ì„ íƒì˜ ê¸°ì¤€"),
    ("ğŸ’° ëˆ/ì†Œë¹„", "ì˜ˆì‚°, ì†Œë¹„ ìŠµê´€, íˆ¬ì/ì €ì¶•, í° êµ¬ë§¤ ê²°ì •"),
    ("ğŸ§  ë§ˆìŒ/ì‚¶", "ë¶ˆì•ˆ/ë²ˆì•„ì›ƒ, ê°€ì¹˜ê´€, ì¸ìƒ ë°©í–¥, ë£¨í‹´/ê· í˜•"),
    ("ğŸ“¦ ê¸°íƒ€", "ì •ë¦¬ë˜ì§€ ì•Šì€ ê³ ë¯¼, ì¼ìƒ ì„ íƒ, ê¸°íƒ€"),
]

DECISION_TYPES = [
    "A vs B ì„ íƒ(ë‘˜ ì¤‘ í•˜ë‚˜)",
    "ì—¬ëŸ¬ ì˜µì…˜ ì¤‘ ì„ íƒ",
    "í•´ì•¼ í• ì§€ ë§ì§€(Yes/No)",
    "ì–¸ì œ/ì–´ë–»ê²Œ í• ì§€(ì „ëµ/ì‹œì )",
    "ê°ˆë“± í•´ê²°/ëŒ€í™” ë°©í–¥",
]

COACHES = [
    {
        "id": "logic",
        "name": "ğŸ” ë…¼ë¦¬ ì½”ì¹˜",
        "tagline": "ì •ë³´ë¥¼ êµ¬ì¡°í™”í•´ì„œ ê²°ì •ì„ ëª…ë£Œí•˜ê²Œ ë•ìŠµë‹ˆë‹¤",
        "style": "ë…¼ë¦¬ì /ê°„ê²°/í”„ë ˆì„ì›Œí¬ ì¤‘ì‹¬",
        "method": [
            "í•µì‹¬ ìŸì Â·ì œì•½ì¡°ê±´ ì •ì˜",
            "ì˜µì…˜/ê¸°ì¤€/ê°€ì¤‘ì¹˜ ì •ë¦¬",
            "ì¥ë‹¨ì Â·ë¦¬ìŠ¤í¬Â·ê°€ì • ê²€ì¦",
            "ê²°ë¡  + ì„ íƒ ê·¼ê±°",
        ],
        "prompt_hint": "MECE, ì˜ì‚¬ê²°ì • ê¸°ì¤€í‘œ, ë¦¬ìŠ¤í¬/ê°€ì • ê²€ì¦ ì§ˆë¬¸",
    },
    {
        "id": "value",
        "name": "ğŸ’— ê°€ì¹˜/ê°ì • ì½”ì¹˜",
        "tagline": "ê°ì •ê³¼ ê°€ì¹˜ê´€ì„ ëª…ë£Œí™”í•´ â€˜ë‚˜ë‹¤ìš´ ì„ íƒâ€™ì„ ë•ìŠµë‹ˆë‹¤",
        "style": "ê³µê°/ê°€ì¹˜ê´€/ê°ì • ëª…ë£Œí™”",
        "method": [
            "ê°ì •/ë‘ë ¤ì›€/ê¸°ëŒ€ ë¶„í•´",
            "ì§„ì§œ ì›í•˜ëŠ” ê²ƒ(ê°€ì¹˜) ë°œêµ´",
            "í›„íšŒ ìµœì†Œí™” ê´€ì (ë¯¸ë˜ì˜ ë‚˜) ì§ˆë¬¸",
            "ë‚˜ë‹µê²Œ ì„ íƒí•˜ëŠ” ë¬¸ì¥ ë§Œë“¤ê¸°",
        ],
        "prompt_hint": "ê°ì • ë¼ë²¨ë§, ê°€ì¹˜ ìš°ì„ ìˆœìœ„, í›„íšŒ í…ŒìŠ¤íŠ¸",
    },
    {
        "id": "action",
        "name": "âš”ï¸ ì‹¤í–‰ ì½”ì¹˜",
        "tagline": "ê²°ì •ì„ ì‹¤í–‰ ê°€ëŠ¥í•œ í–‰ë™ìœ¼ë¡œ ë°”ê¿‰ë‹ˆë‹¤",
        "style": "êµ¬ì²´ì /ì‹¤í–‰/ì‘ì€ ì‹¤í—˜",
        "method": [
            "7ì¼ ì•ˆì— í•  ìˆ˜ ìˆëŠ” ì‹¤í—˜ ì„¤ê³„",
            "ìµœì†Œ í–‰ë™(15ë¶„) + ì²´í¬ë¦¬ìŠ¤íŠ¸",
            "ì¥ì• ë¬¼/ëŒ€ì‘ê³„íš(If-Then)",
            "ì‹¤í–‰ í›„ ë¦¬ë·° ì§ˆë¬¸",
        ],
        "prompt_hint": "ì‘ì€ ì‹¤í—˜, ì¼ì •/ë£¨í‹´, ì¥ì• ë¬¼ ëŒ€ì‘",
    },
]

STEPS = ["ì£¼ì œ ì„ íƒ", "ê³ ë¯¼ ì •ë¦¬(1)", "ê³ ë¯¼ ì •ë¦¬(2)", "ê¸°ì¤€Â·ì˜µì…˜", "ìµœì¢… ì •ë¦¬"]


# =========================
# Pebble (Rock) UI: SVG â†’ base64 HTML img
# =========================
def _pebble_svg(fill: str, shine: str, stroke: str = "#3a3a3a") -> str:
    return f"""
<svg width="160" height="120" viewBox="0 0 160 120" xmlns="http://www.w3.org/2000/svg" role="img" aria-label="pebble">
  <defs>
    <radialGradient id="g" cx="35%" cy="25%" r="80%">
      <stop offset="0%" stop-color="{shine}" stop-opacity="0.95"/>
      <stop offset="55%" stop-color="{fill}" stop-opacity="1"/>
      <stop offset="100%" stop-color="{fill}" stop-opacity="1"/>
    </radialGradient>
  </defs>
  <path d="M28 80
           C14 60, 20 30, 50 20
           C66 14, 96 12, 114 24
           C142 44, 150 68, 130 88
           C112 108, 54 110, 28 80 Z"
        fill="url(#g)" stroke="{stroke}" stroke-width="2" />
  <path d="M54 30 C68 22, 82 22, 94 30"
        fill="none" stroke="{shine}" stroke-width="7" stroke-linecap="round" opacity="0.55"/>
</svg>
""".strip()


def pebble_svg_b64(progress_0_to_1: float, inactive: bool = False) -> str:
    p = max(0.0, min(1.0, float(progress_0_to_1)))
    if inactive:
        fill = "#2f3136"
        shine = "#6b6f7a"
    else:
        fill = "#5f6672" if p < 0.25 else "#707888" if p < 0.5 else "#8892a6" if p < 0.75 else "#a6b2c8"
        shine = "#aab8ff" if p < 0.25 else "#c8d3ff" if p < 0.5 else "#e3e8ff" if p < 0.75 else "#ffffff"
    svg = _pebble_svg(fill=fill, shine=shine)
    return base64.b64encode(svg.encode("utf-8")).decode("ascii")


def render_pebble_row(step_idx: int, total: int) -> None:
    cols = st.columns(total)
    for i in range(total):
        active = i <= step_idx
        p = (i + 1) / total
        b64 = pebble_svg_b64(p, inactive=not active)
        html = f"""
        <div style="text-align:center;">
          <img src="data:image/svg+xml;base64,{b64}" style="width:100%; max-width:150px;"/>
          <div style="font-size:12px; margin-top:4px; opacity:{1.0 if active else 0.55};">
            {STEPS[i]}
          </div>
        </div>
        """
        cols[i].markdown(html, unsafe_allow_html=True)


def render_hero_pebble(progress: float, label: str) -> None:
    b64 = pebble_svg_b64(progress, inactive=False)
    html = f"""
    <div style="text-align:center;">
      <img src="data:image/svg+xml;base64,{b64}" style="width:100%; max-width:240px;"/>
      <div style="margin-top:6px; font-size:14px;">
        {label}
      </div>
    </div>
    """
    st.markdown(html, unsafe_allow_html=True)


# =========================
# OpenAI Helpers
# =========================
def get_api_key() -> str:
    try:
        k = st.secrets.get("OPENAI_API_KEY", "")  # type: ignore
        if k:
            return str(k).strip()
    except Exception:
        pass
    return str(st.session_state.get("openai_api_key_input", "")).strip()


def get_client(api_key: str) -> "OpenAI":
    if OpenAI is None:
        raise RuntimeError("openai íŒ¨í‚¤ì§€ê°€ ì„¤ì¹˜ë˜ì–´ ìˆì§€ ì•ŠìŠµë‹ˆë‹¤. `pip install openai`ë¥¼ ì‹¤í–‰í•˜ì„¸ìš”.")
    return OpenAI(api_key=api_key)


def call_openai_text(system: str, user: str, temperature: float = 0.7) -> Tuple[Optional[str], Optional[str], List[str]]:
    debug: List[str] = []
    api_key = get_api_key()
    if not api_key:
        return None, "OpenAI API Keyê°€ í•„ìš”í•©ë‹ˆë‹¤. Secretsì— OPENAI_API_KEYë¥¼ ë„£ê±°ë‚˜ ì‚¬ì´ë“œë°”ì— ì…ë ¥í•˜ì„¸ìš”.", debug

    try:
        client = get_client(api_key)
    except Exception as e:
        return None, str(e), debug

    if hasattr(client, "responses"):
        for model in [MODEL_PRIMARY, MODEL_FALLBACK]:
            try:
                debug.append(f"Responses API / model={model}")
                resp = client.responses.create(
                    model=model,
                    input=[
                        {"role": "system", "content": [{"type": "text", "text": system}]},
                        {"role": "user", "content": [{"type": "text", "text": user}]},
                    ],
                    temperature=temperature,
                )
                if getattr(resp, "output_text", None):
                    return str(resp.output_text).strip(), None, debug

                out_texts: List[str] = []
                for item in getattr(resp, "output", []) or []:
                    for c in getattr(item, "content", []) or []:
                        if getattr(c, "type", None) == "output_text":
                            out_texts.append(getattr(c, "text", ""))
                text = "\n".join([t for t in out_texts if t]).strip()
                if text:
                    return text, None, debug
                raise RuntimeError("ì‘ë‹µ í…ìŠ¤íŠ¸ ì¶”ì¶œ ì‹¤íŒ¨")
            except Exception as e:
                debug.append(f"Responses failed: {type(e).__name__}: {e}")

    for model in [MODEL_PRIMARY, MODEL_FALLBACK]:
        try:
            debug.append(f"Chat Completions / model={model}")
            cc = client.chat.completions.create(
                model=model,
                messages=[{"role": "system", "content": system}, {"role": "user", "content": user}],
                temperature=temperature,
            )
            text = ""
            if cc.choices:
                text = (cc.choices[0].message.content or "").strip()
            if text:
                return text, None, debug
            raise RuntimeError("ë¹ˆ ì‘ë‹µ")
        except Exception as e:
            debug.append(f"Chat failed: {type(e).__name__}: {e}")

    return None, "OpenAI í˜¸ì¶œì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. ì•„ë˜ ë””ë²„ê·¸ ë¡œê·¸ë¥¼ í™•ì¸í•˜ì„¸ìš”.", debug


# =========================
# State
# =========================
def init_state() -> None:
    if "step" not in st.session_state:
        st.session_state.step = 0
    if "category" not in st.session_state:
        st.session_state.category = TOPIC_CATEGORIES[0][0]
    if "decision_type" not in st.session_state:
        st.session_state.decision_type = DECISION_TYPES[0]
    if "coach_id" not in st.session_state:
        st.session_state.coach_id = COACHES[0]["id"]

    if "answers" not in st.session_state:
        st.session_state.answers = []
    if "generated_questions" not in st.session_state:
        st.session_state.generated_questions = {}

    if "final_report" not in st.session_state:
        st.session_state.final_report = None
    if "debug_log" not in st.session_state:
        st.session_state.debug_log = []
    if "openai_api_key_input" not in st.session_state:
        st.session_state.openai_api_key_input = ""


def coach_by_id(coach_id: str) -> Dict[str, Any]:
    for c in COACHES:
        if c["id"] == coach_id:
            return c
    return COACHES[0]


def add_answer(q: str, a: str) -> None:
    st.session_state.answers.append({"q": q, "a": a, "ts": datetime.now().isoformat(timespec="seconds")})


def reset_flow() -> None:
    st.session_state.step = 0
    st.session_state.answers = []
    st.session_state.generated_questions = {}
    st.session_state.final_report = None
    st.session_state.debug_log = []


# =========================
# Prompting
# =========================
def system_prompt_for(coach: Dict[str, Any]) -> str:
    if coach["id"] == "logic":
        return (
            "ë‹¹ì‹ ì€ 'ë…¼ë¦¬ ì½”ì¹˜'ì…ë‹ˆë‹¤. ëª©í‘œëŠ” ì‚¬ìš©ìì˜ ê³ ë¯¼ì„ ì˜ì‚¬ê²°ì • ë¬¸ì œë¡œ êµ¬ì¡°í™”í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤.\n"
            "- ìŸì /ì˜µì…˜/ê¸°ì¤€/ì œì•½/ê°€ì •/ë¦¬ìŠ¤í¬ë¥¼ ë¶„ë¦¬í•´ì„œ ë‹¤ë£¨ì„¸ìš”.\n"
            "- ì§ˆë¬¸ì€ ì§§ê³ , ë‹µë³€ì„ í‘œë¡œ ë§Œë“¤ê¸° ì‰¬ìš´ í˜•íƒœë¡œ êµ¬ì„±í•˜ì„¸ìš”.\n"
        )
    if coach["id"] == "value":
        return (
            "ë‹¹ì‹ ì€ 'ê°€ì¹˜/ê°ì • ì½”ì¹˜'ì…ë‹ˆë‹¤. ëª©í‘œëŠ” ê°ì •ê³¼ ê°€ì¹˜ê´€ì„ ëª…ë£Œí™”í•´ ì‚¬ìš©ìê°€ 'ë‚˜ë‹¤ìš´ ì„ íƒ'ì„ í•˜ë„ë¡ ë•ëŠ” ê²ƒì…ë‹ˆë‹¤.\n"
            "- ê°ì • ë¼ë²¨ë§ + ê·¸ ê°ì •ì˜ ê·¼ì›(ìš•êµ¬/ë‘ë ¤ì›€)ì„ íƒìƒ‰í•˜ì„¸ìš”.\n"
            "- ê°€ì¹˜(ì¤‘ìš”í•œ ê²ƒ)ë¥¼ 3ê°œë¡œ ì¢íˆê³ , í›„íšŒ ìµœì†Œí™” ê´€ì  ì§ˆë¬¸ì„ í¬í•¨í•˜ì„¸ìš”.\n"
        )
    return (
        "ë‹¹ì‹ ì€ 'ì‹¤í–‰ ì½”ì¹˜'ì…ë‹ˆë‹¤. ëª©í‘œëŠ” ê²°ì •ì„ ì‹¤í–‰ ê°€ëŠ¥í•œ ì‹¤í—˜ê³¼ ë‹¤ìŒ í–‰ë™ìœ¼ë¡œ ë°”ê¾¸ëŠ” ê²ƒì…ë‹ˆë‹¤.\n"
        "- 7ì¼ ì•ˆì— í•  ìˆ˜ ìˆëŠ” ì‘ì€ ì‹¤í—˜ 1~2ê°œë¥¼ ì„¤ê³„í•˜ê²Œ í•˜ì„¸ìš”.\n"
        "- ì¥ì• ë¬¼ê³¼ If-Then ëŒ€ì‘ì„ êµ¬ì²´í™”í•˜ì„¸ìš”.\n"
    )


def build_context_block() -> str:
    cat = st.session_state.category
    dtype = st.session_state.decision_type
    answers = st.session_state.answers

    hist = ""
    for i, qa in enumerate(answers[-6:], start=1):
        hist += f"{i}) Q: {qa['q']}\n   A: {qa['a']}\n"

    return textwrap.dedent(f"""
    [ê³ ë¯¼ ì¹´í…Œê³ ë¦¬]
    {cat}

    [ê²°ì • ìœ í˜•]
    {dtype}

    [ì§€ê¸ˆê¹Œì§€ì˜ Q/A (ìµœê·¼ 6ê°œ)]
    {hist if hist.strip() else "(ì•„ì§ ì—†ìŒ)"}
    """).strip()


def previous_questions_text() -> str:
    # ì´ë¯¸ ìƒì„±ëœ ì§ˆë¬¸(ìºì‹œ)ì„ ë‹¨ê³„ ìˆœì„œëŒ€ë¡œ ë‚˜ì—´
    items = []
    for k in sorted(st.session_state.generated_questions.keys()):
        items.append(f"- (step {k}) {st.session_state.generated_questions[k]}")
    return "\n".join(items) if items else "(ì—†ìŒ)"


def question_instruction(step_idx: int, coach: Dict[str, Any]) -> str:
    if step_idx == 1:
        return "ì‚¬ìš©ìì˜ ê³ ë¯¼ì„ í•œ ë¬¸ë‹¨ìœ¼ë¡œ 'ìƒí™©' ì¤‘ì‹¬ìœ¼ë¡œ ì„¤ëª…í•˜ê²Œ ë§Œë“œëŠ” ì§ˆë¬¸ 1ê°œë¥¼ ì‘ì„±í•˜ì„¸ìš”."
    if step_idx == 2:
        return "ì‚¬ìš©ìì˜ 'ì›í•˜ëŠ” ê²°ê³¼/ë‘ë ¤ìš´ ê²°ê³¼/ê°€ì¥ ì¤‘ìš”í•œ ì œì•½'ì„ ë“œëŸ¬ë‚´ëŠ” ì§ˆë¬¸ 1ê°œë¥¼ ì‘ì„±í•˜ì„¸ìš”."
    if step_idx == 3:
        if coach["id"] == "logic":
            return "ì˜µì…˜ 2~4ê°œ + í‰ê°€ ê¸°ì¤€ 3ê°œë¥¼ ë½‘ê²Œ í•˜ëŠ” ì§ˆë¬¸ 1ê°œë¥¼ ì‘ì„±í•˜ì„¸ìš”. ë‹µì€ í‘œë¡œ ë§Œë“¤ê¸° ì¢‹ê²Œ."
        if coach["id"] == "value":
            return "ê°€ì¹˜ ìš°ì„ ìˆœìœ„ ìƒìœ„ 3ê°œ + í›„íšŒ í…ŒìŠ¤íŠ¸(1ë…„/5ë…„)ë¥¼ í•˜ê²Œ í•˜ëŠ” ì§ˆë¬¸ 1ê°œë¥¼ ì‘ì„±í•˜ì„¸ìš”."
        return "ì´ë²ˆ ì£¼ì— í•  ìˆ˜ ìˆëŠ” 'ì‘ì€ ì‹¤í—˜'ì„ ê³ ë¥´ê²Œ í•˜ëŠ” ì§ˆë¬¸ 1ê°œë¥¼ ì‘ì„±í•˜ì„¸ìš”. (ì˜ˆ: 15ë¶„ í–‰ë™/í•˜ë£¨ ì²´í¬)"
    # step 4
    if coach["id"] == "logic":
        return "ê²°ì • ì „ ë§ˆì§€ë§‰ ê²€ì¦ ì§ˆë¬¸ 1ê°œ(ê°€ì •/ë¦¬ìŠ¤í¬/ëŒ€ì•ˆ)ë¥¼ ì‘ì„±í•˜ì„¸ìš”."
    if coach["id"] == "value":
        return "ê²°ì • ë¬¸ì¥ì„ í•œ ì¤„ë¡œ ë§Œë“¤ê²Œ í•˜ëŠ” ì§ˆë¬¸ 1ê°œ(â€˜ë‚˜ëŠ” ___ë¥¼ ìœ„í•´ ___ì„ ì„ íƒí•œë‹¤â€™)ë¥¼ ì‘ì„±í•˜ì„¸ìš”."
    return "ì‹¤í–‰ ì•½ì†ì„ ê³ ì •í•˜ëŠ” ì§ˆë¬¸ 1ê°œ(ì–¸ì œ/ì–´ë””ì„œ/ë¬´ì—‡ì„/ë§‰íˆë©´ ì–´ë–»ê²Œ)ë¥¼ ì‘ì„±í•˜ì„¸ìš”."


def normalize_question(s: str) -> str:
    return " ".join((s or "").strip().split())


def fallback_last_question(coach_id: str) -> str:
    if coach_id == "logic":
        return "ì´ ê²°ì •ì„ ë‚´ë¦¬ê¸° ì „ì— í™•ì¸í•´ì•¼ í•  ê°€ì¥ í° ê°€ì • 1ê°œì™€, ê·¸ ê°€ì •ì´ í‹€ë ¸ì„ ë•Œì˜ ëŒ€ì•ˆ(í”ŒëœB)ì€ ë¬´ì—‡ì¸ê°€ìš”?"
    if coach_id == "value":
        return "â€˜ë‚˜ëŠ” ___ë¥¼ ìœ„í•´ ___ì„ ì„ íƒí•œë‹¤â€™ ë¬¸ì¥ì„ ì™„ì„±í•´ë³´ë©´, ë¹ˆì¹¸ì—ëŠ” ë¬´ì—‡ì´ ë“¤ì–´ê°€ë‚˜ìš”?"
    return "ì´ë²ˆ ì£¼ ì•ˆì— ì‹¤í–‰í•  ì²« í–‰ë™ì„ â€˜ì–¸ì œ/ì–´ë””ì„œ/ëª‡ ë¶„/ë¬´ì—‡ì„â€™ í•œ ë¬¸ì¥ìœ¼ë¡œ ì ì–´ë³´ë©´ ì–´ë–»ê²Œ ë˜ë‚˜ìš”?"


def generate_next_question(step_idx: int) -> Tuple[Optional[str], Optional[str], List[str]]:
    coach = coach_by_id(st.session_state.coach_id)
    system = system_prompt_for(coach)

    prev_qs = previous_questions_text()
    last_q = st.session_state.generated_questions.get(step_idx - 1, "")

    def _prompt(nonce: int) -> str:
        return textwrap.dedent(f"""
        ë‹¹ì‹ ì€ ì‚¬ìš©ìì˜ ìƒê°ì„ ì •ë¦¬í•˜ê¸° ìœ„í•œ 'ë‹¨ í•˜ë‚˜ì˜ ì§ˆë¬¸'ì„ ë§Œë“­ë‹ˆë‹¤.

        ê·œì¹™:
        - ì§ˆë¬¸ì€ 1ê°œë§Œ ì¶œë ¥ (ì„¤ëª…/ë¨¸ë¦¬ë§ ê¸ˆì§€)
        - í•œêµ­ì–´
        - ì´ì „ ì§ˆë¬¸ê³¼ ë™ì¼í•˜ê±°ë‚˜ ë§¤ìš° ìœ ì‚¬í•œ ì§ˆë¬¸ì€ ê¸ˆì§€
        - ì§ˆë¬¸ì˜ ì´ˆì /ê´€ì ì´ ì´ì „ ì§ˆë¬¸ê³¼ ê²¹ì¹˜ì§€ ì•Šê²Œ êµ¬ì„±
        - ì‚¬ìš©ìê°€ ë‹µí•˜ê¸° ì‰½ê²Œ ì˜ˆì‹œ(ê´„í˜¸ 1ì¤„) í—ˆìš©
        - ê¸ˆì§€: ì•„ë˜ "ì´ì „ ì§ˆë¬¸ ëª©ë¡"ì— ìˆëŠ” ë¬¸ì¥ì„ ê·¸ëŒ€ë¡œ/ìœ ì‚¬í•˜ê²Œ ë°˜ë³µ

        [ì´ì „ ì§ˆë¬¸ ëª©ë¡]
        {prev_qs}

        [ì§ì „ ì§ˆë¬¸(ì°¸ê³ )]
        {last_q if last_q else "(ì—†ìŒ)"}

        {build_context_block()}

        [ì´ë²ˆ ë‹¨ê³„ ëª©ì ]
        {question_instruction(step_idx, coach)}

        (nonce={nonce})  # ì¬ìƒì„± ì‹œ ì¤‘ë³µ ë°©ì§€ìš©

        ì´ì œ ì§ˆë¬¸ 1ê°œë§Œ ì¶œë ¥í•˜ì„¸ìš”.
        """).strip()

    # 1ì°¨ ìƒì„±
    q1, err, dbg = call_openai_text(system=system, user=_prompt(nonce=random.randint(1000, 9999)), temperature=0.75)
    if not q1:
        return None, err, dbg

    q1n = normalize_question(q1)
    lastn = normalize_question(last_q)

    # ì¤‘ë³µ/ìœ ì‚¬(ê°„ë‹¨íŒ) ê°ì§€: ë™ì¼ ë¬¸ìì—´ or ì§ì „ ì§ˆë¬¸ì´ í¬í•¨ë˜ëŠ” ê²½ìš°
    is_dup = (q1n == lastn) or (lastn and (q1n in lastn or lastn in q1n))
    if not is_dup:
        return q1.strip(), None, dbg

    # 2ì°¨ ì¬ìƒì„±(ë” ê°•í•˜ê²Œ)
    dbg.append("Detected duplicate with previous question. Regenerating once with stronger constraints.")
    q2, err2, dbg2 = call_openai_text(system=system, user=_prompt(nonce=random.randint(10000, 99999)), temperature=0.85)
    dbg.extend(dbg2)
    if q2:
        q2n = normalize_question(q2)
        is_dup2 = (q2n == lastn) or (lastn and (q2n in lastn or lastn in q2n))
        if not is_dup2:
            return q2.strip(), None, dbg

    # ê·¸ë˜ë„ ì‹¤íŒ¨í•˜ë©´ ì•ˆì „ ì§ˆë¬¸ìœ¼ë¡œ ëŒ€ì²´
    dbg.append("Still duplicated after retry. Using deterministic fallback question.")
    return fallback_last_question(coach["id"]), None, dbg


def generate_final_report() -> Tuple[Optional[str], Optional[str], List[str]]:
    coach = coach_by_id(st.session_state.coach_id)
    system = system_prompt_for(coach)

    if coach["id"] == "logic":
        format_spec = """
ì¶œë ¥ í˜•ì‹:
## í•œ ì¤„ ê²°ë¡ 
- ê²°ë¡ : ...

## ì˜ì‚¬ê²°ì • êµ¬ì¡°
- ìŸì :
- ì˜µì…˜(2~4):
- í‰ê°€ ê¸°ì¤€(3~5):
- ì œì•½/ê°€ì •:
- ë¦¬ìŠ¤í¬/ëŒ€ì‘:

## ì¶”ì²œì•ˆ (ê·¼ê±°)
- ì¶”ì²œ: ...
- ì´ìœ (3ì¤„):
- ë³´ì™„ì±…(ë¦¬ìŠ¤í¬ ì¤„ì´ê¸°):

## ë‹¤ìŒ í–‰ë™(24ì‹œê°„ ë‚´)
- ...
"""
    elif coach["id"] == "value":
        format_spec = """
ì¶œë ¥ í˜•ì‹:
## ì§€ê¸ˆì˜ ë§ˆìŒ ìš”ì•½
- ê°ì •(3ê°œ): ...
- ì§„ì§œ ìš•êµ¬/ë‘ë ¤ì›€: ...

## ë‚˜ì˜ ê¸°ì¤€(ê°€ì¹˜)
- ìƒìœ„ 3ê°€ì§€: ...
- ë‚´ë ¤ë†“ì„ ìˆ˜ ìˆëŠ” ê²ƒ 1ê°€ì§€: ...

## ë‚˜ë‹¤ìš´ ì„ íƒ ë¬¸ì¥
- â€œë‚˜ëŠ” ___ë¥¼ ìœ„í•´ ___ì„ ì„ íƒí•œë‹¤.â€

## í›„íšŒ ìµœì†Œí™” ì²´í¬
- 1ë…„ ë’¤ì˜ ë‚˜: ...
- 5ë…„ ë’¤ì˜ ë‚˜: ...

## ë‚´ì¼ì˜ ì‘ì€ ì•½ì†
- ...
"""
    else:
        format_spec = """
ì¶œë ¥ í˜•ì‹:
## ê²°ì •ì„ í–‰ë™ìœ¼ë¡œ ë°”ê¾¸ê¸°
- ì´ë²ˆ ì£¼ í•µì‹¬ ëª©í‘œ(1ê°œ): ...

## 7ì¼ ì‹¤í—˜(1~2ê°œ)
- ì‹¤í—˜1: (15ë¶„ ë‹¨ìœ„ë¡œ ìª¼ê°œì„œ)
- ì‹¤í—˜2(ì„ íƒ): ...

## If-Then ëŒ€ì‘í‘œ
- ë§Œì•½ ___ì´ë©´ â†’ ___í•œë‹¤ (3ê°œ)

## ì˜¤ëŠ˜(24ì‹œê°„ ë‚´) ì²´í¬ë¦¬ìŠ¤íŠ¸
- [ ] ...
- [ ] ...
- [ ] ...

## ë¦¬ë·° ì§ˆë¬¸(ì‹¤í—˜ í›„)
- ...
"""

    user = textwrap.dedent(f"""
ì•„ë˜ Q/Aë¥¼ ë°”íƒ•ìœ¼ë¡œ, ì½”ì¹˜ ì—­í• ì— ë§ëŠ” 'ìµœì¢… ì •ë¦¬ ë¦¬í¬íŠ¸'ë¥¼ ì‘ì„±í•˜ì„¸ìš”.

ê·œì¹™:
- í•œêµ­ì–´
- ì„ íƒì„ ê°•ìš”í•˜ì§€ ë§ê³ , ê·¼ê±°ì™€ ë‹¤ìŒ ìŠ¤í…ì„ ëª…í™•íˆ
- ë¶ˆí™•ì‹¤í•œ ë¶€ë¶„ì€ 'ì¶”ê°€ í™•ì¸ ì§ˆë¬¸' 1ê°œë¥¼ ë§ˆì§€ë§‰ì— ì œì•ˆ
- ê¸¸ì´: 500~900ì

{build_context_block()}

{format_spec}

ë§ˆì§€ë§‰ ì¤„:
- ì¶”ê°€ í™•ì¸ ì§ˆë¬¸: ...
""").strip()

    return call_openai_text(system=system, user=user, temperature=0.65)


# =========================
# UI
# =========================
init_state()

with st.sidebar:
    st.header("ì„¤ì •")
    st.text_input("OpenAI API Key (Secrets ìš°ì„ )", type="password", key="openai_api_key_input")

    st.divider()
    st.subheader("ê³ ë¯¼ ë²”ìœ„")
    st.selectbox("ê³ ë¯¼ ì¹´í…Œê³ ë¦¬", [x[0] for x in TOPIC_CATEGORIES], key="category")
    st.selectbox("ê²°ì • ìœ í˜•", DECISION_TYPES, key="decision_type")

    st.divider()
    st.subheader("ì½”ì¹˜ ì„ íƒ")
    coach_labels = [f"{c['name']} â€” {c['tagline']}" for c in COACHES]
    current_idx = next((i for i, c in enumerate(COACHES) if c["id"] == st.session_state.coach_id), 0)
    picked = st.radio("ì½”ì¹˜", coach_labels, index=current_idx)
    st.session_state.coach_id = COACHES[coach_labels.index(picked)]["id"]

    coach = coach_by_id(st.session_state.coach_id)
    with st.expander("ì½”ì¹˜ ì§„í–‰ ë°©ì‹"):
        st.markdown(f"**{coach['name']}**  \n_{coach['style']}_")
        for m in coach["method"]:
            st.write(f"- {m}")
        st.caption(f"íŠ¹ì§•: {coach['prompt_hint']}")

    st.divider()
    c1, c2 = st.columns(2)
    with c1:
        if st.button("ìƒˆ ê³ ë¯¼ ì‹œì‘", use_container_width=True):
            reset_flow()
            st.rerun()
    with c2:
        disabled_next = st.session_state.step >= (len(STEPS) - 1)
        if st.button("ë‹¤ìŒ ë‹¨ê³„", use_container_width=True, disabled=disabled_next):
            st.session_state.step += 1
            st.rerun()

st.title("ğŸª¨ ëŒë©©ì´ ê²°ì • ì½”ì¹˜")
st.caption("ì§ˆë¬¸ì— ë‹µí•˜ë©° ìƒê°ì„ ì •ë¦¬í•©ë‹ˆë‹¤. ë‹¨ê³„ê°€ ì§„í–‰ë ìˆ˜ë¡ ì‹œê°ì ìœ¼ë¡œë„ ì§„í–‰ë„ê°€ í‘œì‹œë©ë‹ˆë‹¤.")

render_pebble_row(st.session_state.step, len(STEPS))

progress = st.session_state.step / (len(STEPS) - 1)
with st.columns([1, 2, 1])[1]:
    render_hero_pebble(progress, f"í˜„ì¬ ë‹¨ê³„: {STEPS[st.session_state.step]} Â· ì§„í–‰ë„: {int(progress*100)}%")

st.divider()

coach = coach_by_id(st.session_state.coach_id)

if st.session_state.step == 0:
    st.subheader("1) ë¨¼ì € ê³ ë¯¼ì„ êµ¬ì²´í™”í•©ë‹ˆë‹¤")
    cat_desc = next((d for n, d in TOPIC_CATEGORIES if n == st.session_state.category), "")
    st.info(f"**ì¹´í…Œê³ ë¦¬:** {st.session_state.category}\n\n{cat_desc}")
    st.success("ì‚¬ì´ë“œë°”ì—ì„œ â€˜ë‹¤ìŒ ë‹¨ê³„â€™ë¥¼ ëˆŒëŸ¬ ì§ˆë¬¸ì„ ì‹œì‘í•˜ì„¸ìš”.")

else:
    step_idx = st.session_state.step

    if step_idx not in st.session_state.generated_questions:
        q, err, dbg = generate_next_question(step_idx)
        st.session_state.debug_log = dbg
        if q:
            st.session_state.generated_questions[step_idx] = q
        else:
            st.error(err or "ì§ˆë¬¸ ìƒì„± ì‹¤íŒ¨")
            with st.expander("ë””ë²„ê·¸ ë¡œê·¸"):
                st.write(dbg)
            st.stop()

    question = st.session_state.generated_questions[step_idx]

    with st.container(border=True):
        st.markdown(f"### ì§ˆë¬¸ {step_idx} (ì½”ì¹˜: {coach['name']})")
        st.markdown(f"**Q. {question}**")

    with st.form(f"answer_form_{step_idx}", clear_on_submit=True):
        hint = ""
        if st.session_state.answers:
            last_a = st.session_state.answers[-1]["a"]
            hint = f"ì´ì „ ë‹µ ìš”ì•½: {last_a[:90]}{'â€¦' if len(last_a) > 90 else ''}"
        answer = st.text_area("ë‹µë³€", placeholder=hint or "ì—¬ê¸°ì— ë‹µë³€ì„ ì…ë ¥í•˜ì„¸ìš”", height=140)
        submitted = st.form_submit_button("ë‹µë³€ ì €ì¥í•˜ê³  ì§„í–‰", use_container_width=True)

    if submitted:
        if not answer.strip():
            st.warning("ë‹µë³€ì´ ë¹„ì–´ ìˆìŠµë‹ˆë‹¤. í•œ ì¤„ë§Œ ì…ë ¥í•´ë„ ì§„í–‰ ê°€ëŠ¥í•©ë‹ˆë‹¤.")
        else:
            add_answer(question, answer.strip())
            st.success("ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
            if st.session_state.step < len(STEPS) - 1:
                st.session_state.step += 1
            st.rerun()

    st.subheader("ë‹µë³€ ê¸°ë¡")
    if not st.session_state.answers:
        st.caption("ì•„ì§ ë‹µë³€ì´ ì—†ìŠµë‹ˆë‹¤.")
    else:
        for i, qa in enumerate(st.session_state.answers, start=1):
            with st.expander(f"Q{i}. {qa['q']}"):
                st.write(qa["a"])
                st.caption(qa["ts"])

    if st.session_state.step == len(STEPS) - 1:
        st.divider()
        st.subheader("ìµœì¢… ì •ë¦¬ ë¦¬í¬íŠ¸")

        gen = st.button("ìµœì¢… ë¦¬í¬íŠ¸ ìƒì„±", type="primary", use_container_width=True)
        if gen:
            with st.spinner("ë¦¬í¬íŠ¸ë¥¼ ìƒì„±í•˜ëŠ” ì¤‘..."):
                report, err, dbg = generate_final_report()
                st.session_state.debug_log = dbg
                if report:
                    st.session_state.final_report = report
                else:
                    st.session_state.final_report = None
                    st.error(err or "ë¦¬í¬íŠ¸ ìƒì„± ì‹¤íŒ¨")

        if st.session_state.final_report:
            st.success("ë¦¬í¬íŠ¸ê°€ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤.")
            st.markdown(st.session_state.final_report)

            st.markdown("ê³µìœ ìš© ìš”ì•½(JSON)")
            share = {
                "category": st.session_state.category,
                "decision_type": st.session_state.decision_type,
                "coach": coach["name"],
                "questions": st.session_state.generated_questions,
                "answers": st.session_state.answers,
                "final_report": st.session_state.final_report,
            }
            st.code(json.dumps(share, ensure_ascii=False, indent=2), language="json")

    with st.expander("ë””ë²„ê·¸ ë¡œê·¸(ë¬¸ì œ ë°œìƒ ì‹œ í™•ì¸)"):
        st.write(st.session_state.debug_log)

st.divider()
with st.expander("Streamlit Cloud ë°°í¬ ì²´í¬ë¦¬ìŠ¤íŠ¸"):
    st.markdown(
        """
- Secrets ì„¤ì •: Settings â†’ Secretsì— `OPENAI_API_KEY = "sk-..."` ì¶”ê°€
- requirements.txt:
  - streamlit
  - openai
- ëª¨ë¸ ê¶Œí•œ ë¬¸ì œê°€ ìˆìœ¼ë©´ ìë™ìœ¼ë¡œ gpt-4o-minië¡œ ì¬ì‹œë„í•©ë‹ˆë‹¤.
"""
    )
